{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "mount_file_id": "1BSMFuCSAYv269PQMTXCSByX9ghKIbcGL",
      "authorship_tag": "ABX9TyNDWQ9+TPaGkZpKr7fmRSwf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DONDAJIN/Kaggle_amex/blob/master/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wDuO2F6Tqxit"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import joblib\n",
        "import random\n",
        "import warnings\n",
        "import itertools\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import lightgbm as lgb\n",
        "from itertools import combinations\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings; warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    seed = 58\n",
        "    n_folds = 5\n",
        "    target = 'target'\n",
        "    DIR='/content/drive/MyDrive/Kaggle'\n",
        "    INPUT= os.path.join(DIR,'Input')\n",
        "    OUTPUT=os.path.join(DIR,'Output')\n",
        "    EXP=os.path.join(DIR,'exp/exp003')\n",
        "    MODEL=os.path.join(EXP,'Model')\n",
        "    LOG=os.path.join(EXP,'Log')\n",
        "    PRED=os.path.join(EXP,'pred')\n",
        "    os.makedirs(EXP,exist_ok=True)\n",
        "    for i in ['Log','Model','pred']:\n",
        "      os.makedirs(os.path.join(EXP,i),exist_ok=True)\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "def read_data():\n",
        "    train = pd.read_parquet(os.path.join(Config.INPUT,'train_fe_plus_plus.parquet'))\n",
        "    test = pd.read_parquet(os.path.join(Config.INPUT,'test_fe_plus_plus.parquet'))\n",
        "    return train, test\n",
        "\n",
        "def amex_metric(y_true, y_pred):\n",
        "    labels = np.transpose(np.array([y_true, y_pred]))\n",
        "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
        "    weights = np.where(labels[:,0]==0, 20, 1)\n",
        "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
        "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
        "    gini = [0,0]\n",
        "    for i in [1,0]:\n",
        "        labels = np.transpose(np.array([y_true, y_pred]))\n",
        "        labels = labels[labels[:, i].argsort()[::-1]]\n",
        "        weight = np.where(labels[:,0]==0, 20, 1)\n",
        "        weight_random = np.cumsum(weight / np.sum(weight))\n",
        "        total_pos = np.sum(labels[:, 0] *  weight)\n",
        "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
        "        lorentz = cum_pos_found / total_pos\n",
        "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
        "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
        "\n",
        "def amex_metric_np(preds, target):\n",
        "    indices = np.argsort(preds)[::-1]\n",
        "    preds, target = preds[indices], target[indices]\n",
        "    weight = 20.0 - target * 19.0\n",
        "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
        "    four_pct_mask = cum_norm_weight <= 0.04\n",
        "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
        "    weighted_target = target * weight\n",
        "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
        "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
        "    n_pos = np.sum(target)\n",
        "    n_neg = target.shape[0] - n_pos\n",
        "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
        "    g = gini / gini_max\n",
        "    return 0.5 * (g + d)"
      ],
      "metadata": {
        "id": "vVK2MnWOq2iA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_difference(data,num_features):\n",
        "  df1=[]\n",
        "  customer_ids=[]\n",
        "  for customer_id,df in tqdm(data.groupby(['customer_ID'])): #groupbyオブジェクトfor文で回せる\n",
        "    diff_df1=df[num_features].diff(1).iloc[[-1]].values.astype('float32') #各customerの各列の差の最後の行\n",
        "    df1.append(diff_df1)\n",
        "    customer_ids.append(customer_id)\n",
        "  df1=np.concatenate(df1,axis=0)\n",
        "  df1=pd.DataFrame(df1,columns=[col+'_diff1' for col in df[num_features].columns])\n",
        "  df1['customer_ID']=customer_ids\n",
        "  return df1"
      ],
      "metadata": {
        "id": "Ef8hazo1q566"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_preprocess_data():\n",
        "  test=pd.read_parquet(os.path.join(Config.INPUT,'test_rounded.parquet'))\n",
        "  features=test.drop(['customer_ID','S_2'],axis=1).columns.to_list() #inplace=False\n",
        "  cat_features=[\n",
        "      'B_30',\n",
        "      'B_38',\n",
        "      'D_114',\n",
        "      'D_116',\n",
        "      'D_117',\n",
        "      'D_120',\n",
        "      'D_126',\n",
        "      'D_63',\n",
        "      'D_64',\n",
        "      'D_66',\n",
        "      'D_68',\n",
        "  ]\n",
        "  num_features=[col for col in features if col not in cat_features]\n",
        "\n",
        "  #Test feature engineering\n",
        "  test=pd.read_parquet(os.path.join(Config.INPUT,'test_rounded.parquet'))\n",
        "  print('Start test feature engineering------------')\n",
        "  test_num_agg=test.groupby('customer_ID')[num_features].agg(['first','mean','std','min','max','last'])\n",
        "  test_num_agg.columns=['_'.join(x) for x in test_num_agg.columns]\n",
        "  test_num_agg.reset_index(inplace=True)\n",
        "\n",
        "  #Lag features\n",
        "  for col in test_num_agg:\n",
        "    if 'last' in col and col.replace('last','first') in test_num_agg:\n",
        "      test_num_agg[col+'_lag_sub']=test_num_agg[col]-test_num_agg[col.replace('last','first')]\n",
        "      test_num_agg[col+'_lag_div']=test_num_agg[col]/test_num_agg[col.replace('last','first')]\n",
        "\n",
        "  test_cat_agg=test.groupby('customer_ID')[cat_features].agg(['count','first','last','nunique'])\n",
        "  test_cat_agg.columns=['_'.join(x) for x in test_cat_agg.columns]\n",
        "  test_cat_agg.reset_index(inplace=True)\n",
        "\n",
        "  cols=list(test_num_agg.dtypes[test_num_agg.dtypes=='float64'].index) #dtypesではindexが列名\n",
        "  for col in tqdm(cols):\n",
        "    test_num_agg[col]=test_num_agg[col].astype(np.float32)\n",
        "\n",
        "  cols=list(test_cat_agg.dtypes[test_cat_agg.dtypes=='int64'].index)\n",
        "  for col in tqdm(cols):\n",
        "    test_cat_agg[col]=test_cat_agg[col].astype(np.int32)\n",
        "  \n",
        "  test_diff=get_difference(test,num_features)\n",
        "  test=test_num_agg.merge(test_cat_agg,how='inner',on='customer_ID').merge(test_diff,how='inner',on='customer_ID')\n",
        "  del test_num_agg,test_cat_agg,test_diff\n",
        "  gc.collect()\n",
        "  test.to_parquet(os.path.join(Config.INPUT,'test_fe_plus_plus.parquet'))"
      ],
      "metadata": {
        "id": "MK2NgcYIq8pq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lgb_amex_metric(y_pred, y_true):\n",
        "    y_true = y_true.get_label()\n",
        "    return 'amex_metric', amex_metric(y_true, y_pred), True"
      ],
      "metadata": {
        "id": "rbBC9adsrB-H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(train,test):\n",
        "  # Label encoding\n",
        "  cat_features=[\n",
        "      'B_30',\n",
        "      'B_38',\n",
        "      'D_114',\n",
        "      'D_116',\n",
        "      'D_117',\n",
        "      'D_120',\n",
        "      'D_126',\n",
        "      'D_63',\n",
        "      'D_64',\n",
        "      'D_66',\n",
        "      'D_68'\n",
        "      ]\n",
        "  cat_features=[f'{cf}_last' for cf in cat_features] #Preprocess後の'B_30_last'などにLabel encodingする\n",
        "  for cat_col in cat_features:\n",
        "    encoder=LabelEncoder()\n",
        "    train[cat_col]=encoder.fit_transform(train[cat_col])\n",
        "    test[cat_col]=encoder.transform(test[cat_col])\n",
        "\n",
        "  #Round last float features to 2 decimal place\n",
        "  num_cols=list(train.dtypes[(train.dtypes=='float32') | (train.dtypes=='float64')].index)\n",
        "  num_cols=[col for col in num_cols if 'last' in col] #lastが付いている列だけ\n",
        "  for col in num_cols:\n",
        "    train[col+'_round2']=train[col].round(2)\n",
        "    test[col+'_round2']=test[col].round(2)\n",
        "  \n",
        "  #Get the difference between last and mean\n",
        "  num_cols=[col for col in train.columns if 'last' in col] #lastが付いている列だけを取り出して\n",
        "  num_cols=[col[:-5] for col in num_cols if 'round' not in col] # 'B_2_last'から'B_2'\n",
        "  for col in num_cols:\n",
        "    try:\n",
        "      train[f'{col}_last_mean_diff']=train[f'{col}_last']-train[f'{col}_mean']\n",
        "      test[f'{col}_last_mean_diff']=test[f'{col}_last']-test[f'{col}_mean']\n",
        "    except:\n",
        "      pass\n",
        "  \n",
        "  #Transform float 64 and float 32 to float 16\n",
        "  num_cols=list(train.dtypes[(train.dtypes=='foat32') | (train.dtypes=='float16')].index)\n",
        "  for col in tqdm(num_cols):\n",
        "    train[col]=train[col].astype(np.float16)\n",
        "    test[col]=test[col].astype(np.float16)\n",
        "  #Get feature list\n",
        "  features=[col for col in train.columns if col not in ['customer_ID',Config.target]]\n",
        "  params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': \"None\",\n",
        "        'boosting': 'dart',\n",
        "        'seed': Config.seed,\n",
        "        'num_leaves': 100,\n",
        "        'learning_rate': 0.01,\n",
        "        'feature_fraction': 0.20,\n",
        "        'bagging_freq': 10,\n",
        "        'bagging_fraction': 0.50,\n",
        "        'n_jobs': -1,\n",
        "        'lambda_l2': 2,\n",
        "        'min_data_in_leaf': 40\n",
        "        }\n",
        "  #Creat a numpy array to store test,oof predictions\n",
        "  test_predictions=np.zeros(len(test))\n",
        "  oof_predictions=np.zeros(len(train))\n",
        "\n",
        "  kfold=StratifiedKFold(n_splits=Config.n_folds,shuffle=True,random_state=Config.seed)\n",
        "  for fold,(tr_idx,val_idx) in enumerate(kfold.split(train,train[Config.target])):\n",
        "    print(' ')\n",
        "    print('-'*50)\n",
        "    print(f'Training fold {fold} with {len(features)} features...')\n",
        "    X_tr,X_val=train[features].iloc[tr_idx],train[features].iloc[val_idx]\n",
        "    y_tr,y_val=train[Config.target].iloc[tr_idx],train[Config.target].iloc[val_idx]\n",
        "\n",
        "    lgb_train=lgb.Dataset(X_tr,y_tr,categorical_feature=cat_features)\n",
        "    lgb_valid=lgb.Dataset(X_val,y_val,categorical_feature=cat_features)\n",
        "    model=lgb.train(\n",
        "        params=params,\n",
        "        train_set=lgb_train,\n",
        "        num_boost_round=9500,\n",
        "        valid_sets=[lgb_train,lgb_valid],\n",
        "        early_stopping_rounds=100,\n",
        "        verbose_eval=500,\n",
        "        feval=lgb_amex_metric,\n",
        "        fobj=None\n",
        "        )\n",
        "    #Save best model\n",
        "    joblib.dump(model,os.path.join(Config.MODEL,f'lgbm_fold{fold}_seed{Config.seed}.pkl'))\n",
        "\n",
        "    #Predict validation\n",
        "    val_pred=model.predict(X_val)\n",
        "\n",
        "    #Add to oof arrray\n",
        "    oof_predictions[val_idx]=val_pred\n",
        "\n",
        "    \n",
        "    #Compute fold metric\n",
        "    score=amex_metric(y_val,val_pred)\n",
        "    print(f'Our Fold {fold} CV score is {score} ')\n",
        "    del X_tr,X_val,y_tr,y_val,lgb_train,lgb_valid\n",
        "    gc.collect()\n",
        "  \n",
        "  #Compute oof metric\n",
        "  score=amex_metric(train[Config.target],oof_predictions)\n",
        "  print(f'Our out of folds CV Score is {score}')\n",
        "\n",
        "  #Create a dataframe to store out of folds predictions\n",
        "  oof_df=pd.DataFrame({'customer_ID':train['customer_ID'],'target':train[Config.target],'Prediction':oof_predictions})\n",
        "  oof_df.to_csv(os.path.join(Config.PRED,'oof_lgbm_baseline_{Config.n_folds}fold_seed{Config.seed}.csv'),index=False)\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "NT8ggdxDrEaN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seed_everything(Config.seed)\n",
        "#read_preprocess_data()\n",
        "train,test=read_data()\n",
        "train_and_evaluate(train,test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SW2jFkYCrHl9",
        "outputId": "25eb64b5-f1cd-42f8-82f1-105b200703eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "--------------------------------------------------\n",
            "Training fold 0 with 2177 features...\n",
            "[500]\ttraining's amex_metric: 0.781145\tvalid_1's amex_metric: 0.769182\n",
            "[1000]\ttraining's amex_metric: 0.793568\tvalid_1's amex_metric: 0.778786\n",
            "[1500]\ttraining's amex_metric: 0.806847\tvalid_1's amex_metric: 0.785046\n",
            "[2000]\ttraining's amex_metric: 0.818955\tvalid_1's amex_metric: 0.789775\n",
            "[2500]\ttraining's amex_metric: 0.831714\tvalid_1's amex_metric: 0.793729\n",
            "[3000]\ttraining's amex_metric: 0.842054\tvalid_1's amex_metric: 0.796304\n",
            "[3500]\ttraining's amex_metric: 0.850504\tvalid_1's amex_metric: 0.797093\n",
            "[4000]\ttraining's amex_metric: 0.85941\tvalid_1's amex_metric: 0.797707\n",
            "[4500]\ttraining's amex_metric: 0.869816\tvalid_1's amex_metric: 0.798946\n",
            "[5000]\ttraining's amex_metric: 0.878342\tvalid_1's amex_metric: 0.799765\n",
            "[5500]\ttraining's amex_metric: 0.887616\tvalid_1's amex_metric: 0.800506\n",
            "[6000]\ttraining's amex_metric: 0.895494\tvalid_1's amex_metric: 0.800645\n",
            "[6500]\ttraining's amex_metric: 0.902432\tvalid_1's amex_metric: 0.800268\n",
            "[7000]\ttraining's amex_metric: 0.909341\tvalid_1's amex_metric: 0.801357\n",
            "[7500]\ttraining's amex_metric: 0.915653\tvalid_1's amex_metric: 0.801033\n",
            "[8000]\ttraining's amex_metric: 0.921232\tvalid_1's amex_metric: 0.800614\n",
            "[8500]\ttraining's amex_metric: 0.927637\tvalid_1's amex_metric: 0.800708\n",
            "[9000]\ttraining's amex_metric: 0.933201\tvalid_1's amex_metric: 0.800784\n",
            "[9500]\ttraining's amex_metric: 0.938871\tvalid_1's amex_metric: 0.800501\n",
            "Our Fold 0 CV score is 0.8010446253088215 \n",
            " \n",
            "--------------------------------------------------\n",
            "Training fold 1 with 2177 features...\n",
            "[500]\ttraining's amex_metric: 0.781072\tvalid_1's amex_metric: 0.769196\n",
            "[1000]\ttraining's amex_metric: 0.793802\tvalid_1's amex_metric: 0.776336\n",
            "[1500]\ttraining's amex_metric: 0.807309\tvalid_1's amex_metric: 0.785473\n",
            "[2000]\ttraining's amex_metric: 0.819282\tvalid_1's amex_metric: 0.78966\n",
            "[2500]\ttraining's amex_metric: 0.831467\tvalid_1's amex_metric: 0.793733\n",
            "[3000]\ttraining's amex_metric: 0.842464\tvalid_1's amex_metric: 0.795831\n",
            "[3500]\ttraining's amex_metric: 0.85088\tvalid_1's amex_metric: 0.796072\n",
            "[4000]\ttraining's amex_metric: 0.859815\tvalid_1's amex_metric: 0.796151\n",
            "[4500]\ttraining's amex_metric: 0.869666\tvalid_1's amex_metric: 0.796529\n",
            "[5000]\ttraining's amex_metric: 0.878165\tvalid_1's amex_metric: 0.797901\n",
            "[5500]\ttraining's amex_metric: 0.887088\tvalid_1's amex_metric: 0.798456\n",
            "[6000]\ttraining's amex_metric: 0.895027\tvalid_1's amex_metric: 0.79898\n",
            "[6500]\ttraining's amex_metric: 0.902036\tvalid_1's amex_metric: 0.798991\n",
            "[7000]\ttraining's amex_metric: 0.908504\tvalid_1's amex_metric: 0.799399\n",
            "[7500]\ttraining's amex_metric: 0.914963\tvalid_1's amex_metric: 0.799325\n",
            "[8000]\ttraining's amex_metric: 0.920915\tvalid_1's amex_metric: 0.799063\n",
            "[8500]\ttraining's amex_metric: 0.927034\tvalid_1's amex_metric: 0.798972\n",
            "[9000]\ttraining's amex_metric: 0.932871\tvalid_1's amex_metric: 0.799024\n",
            "[9500]\ttraining's amex_metric: 0.938837\tvalid_1's amex_metric: 0.799375\n",
            "Our Fold 1 CV score is 0.7994290138264433 \n",
            " \n",
            "--------------------------------------------------\n",
            "Training fold 2 with 2177 features...\n",
            "[500]\ttraining's amex_metric: 0.783147\tvalid_1's amex_metric: 0.763393\n",
            "[1000]\ttraining's amex_metric: 0.795662\tvalid_1's amex_metric: 0.770672\n",
            "[1500]\ttraining's amex_metric: 0.80855\tvalid_1's amex_metric: 0.77771\n",
            "[2000]\ttraining's amex_metric: 0.820292\tvalid_1's amex_metric: 0.782888\n",
            "[2500]\ttraining's amex_metric: 0.832628\tvalid_1's amex_metric: 0.787043\n",
            "[3000]\ttraining's amex_metric: 0.843824\tvalid_1's amex_metric: 0.789441\n",
            "[3500]\ttraining's amex_metric: 0.852394\tvalid_1's amex_metric: 0.791213\n",
            "[4000]\ttraining's amex_metric: 0.861203\tvalid_1's amex_metric: 0.791497\n",
            "[4500]\ttraining's amex_metric: 0.871287\tvalid_1's amex_metric: 0.79189\n",
            "[5000]\ttraining's amex_metric: 0.879445\tvalid_1's amex_metric: 0.792757\n",
            "[5500]\ttraining's amex_metric: 0.888593\tvalid_1's amex_metric: 0.792842\n",
            "[6000]\ttraining's amex_metric: 0.896733\tvalid_1's amex_metric: 0.793571\n",
            "[6500]\ttraining's amex_metric: 0.903883\tvalid_1's amex_metric: 0.792589\n",
            "[7000]\ttraining's amex_metric: 0.910506\tvalid_1's amex_metric: 0.793085\n",
            "[7500]\ttraining's amex_metric: 0.916589\tvalid_1's amex_metric: 0.793128\n",
            "[8000]\ttraining's amex_metric: 0.922076\tvalid_1's amex_metric: 0.792925\n",
            "[8500]\ttraining's amex_metric: 0.928255\tvalid_1's amex_metric: 0.7936\n",
            "[9000]\ttraining's amex_metric: 0.933589\tvalid_1's amex_metric: 0.793534\n",
            "[9500]\ttraining's amex_metric: 0.939054\tvalid_1's amex_metric: 0.792881\n",
            "Our Fold 2 CV score is 0.7933658778290426 \n",
            " \n",
            "--------------------------------------------------\n",
            "Training fold 3 with 2177 features...\n",
            "[500]\ttraining's amex_metric: 0.781327\tvalid_1's amex_metric: 0.768435\n",
            "[1000]\ttraining's amex_metric: 0.793921\tvalid_1's amex_metric: 0.775308\n",
            "[1500]\ttraining's amex_metric: 0.80764\tvalid_1's amex_metric: 0.780826\n",
            "[2000]\ttraining's amex_metric: 0.820157\tvalid_1's amex_metric: 0.786196\n",
            "[2500]\ttraining's amex_metric: 0.832655\tvalid_1's amex_metric: 0.790205\n",
            "[3000]\ttraining's amex_metric: 0.843643\tvalid_1's amex_metric: 0.791596\n",
            "[3500]\ttraining's amex_metric: 0.85154\tvalid_1's amex_metric: 0.791856\n",
            "[4000]\ttraining's amex_metric: 0.860144\tvalid_1's amex_metric: 0.792673\n",
            "[4500]\ttraining's amex_metric: 0.870297\tvalid_1's amex_metric: 0.793352\n",
            "[5000]\ttraining's amex_metric: 0.87889\tvalid_1's amex_metric: 0.794344\n",
            "[5500]\ttraining's amex_metric: 0.887968\tvalid_1's amex_metric: 0.79497\n",
            "[6000]\ttraining's amex_metric: 0.895893\tvalid_1's amex_metric: 0.795512\n",
            "[6500]\ttraining's amex_metric: 0.902513\tvalid_1's amex_metric: 0.796217\n",
            "[7000]\ttraining's amex_metric: 0.909367\tvalid_1's amex_metric: 0.795548\n",
            "[7500]\ttraining's amex_metric: 0.915571\tvalid_1's amex_metric: 0.796111\n",
            "[8000]\ttraining's amex_metric: 0.921231\tvalid_1's amex_metric: 0.795926\n",
            "[8500]\ttraining's amex_metric: 0.927271\tvalid_1's amex_metric: 0.79525\n",
            "[9000]\ttraining's amex_metric: 0.933294\tvalid_1's amex_metric: 0.795931\n",
            "[9500]\ttraining's amex_metric: 0.938715\tvalid_1's amex_metric: 0.795752\n",
            "Our Fold 3 CV score is 0.7955348141954321 \n",
            " \n",
            "--------------------------------------------------\n",
            "Training fold 4 with 2177 features...\n",
            "[500]\ttraining's amex_metric: 0.780748\tvalid_1's amex_metric: 0.769761\n",
            "[1000]\ttraining's amex_metric: 0.793378\tvalid_1's amex_metric: 0.778173\n",
            "[1500]\ttraining's amex_metric: 0.806992\tvalid_1's amex_metric: 0.784657\n",
            "[2000]\ttraining's amex_metric: 0.818974\tvalid_1's amex_metric: 0.78802\n",
            "[2500]\ttraining's amex_metric: 0.831372\tvalid_1's amex_metric: 0.791269\n",
            "[3000]\ttraining's amex_metric: 0.842555\tvalid_1's amex_metric: 0.794563\n",
            "[3500]\ttraining's amex_metric: 0.850497\tvalid_1's amex_metric: 0.795495\n",
            "[4000]\ttraining's amex_metric: 0.85942\tvalid_1's amex_metric: 0.796306\n",
            "[4500]\ttraining's amex_metric: 0.870136\tvalid_1's amex_metric: 0.797476\n",
            "[5000]\ttraining's amex_metric: 0.878732\tvalid_1's amex_metric: 0.797917\n",
            "[5500]\ttraining's amex_metric: 0.887665\tvalid_1's amex_metric: 0.798725\n",
            "[6000]\ttraining's amex_metric: 0.895834\tvalid_1's amex_metric: 0.799475\n",
            "[6500]\ttraining's amex_metric: 0.902931\tvalid_1's amex_metric: 0.800139\n",
            "[7000]\ttraining's amex_metric: 0.909449\tvalid_1's amex_metric: 0.800099\n",
            "[7500]\ttraining's amex_metric: 0.915709\tvalid_1's amex_metric: 0.799793\n",
            "[8000]\ttraining's amex_metric: 0.921572\tvalid_1's amex_metric: 0.799777\n",
            "[8500]\ttraining's amex_metric: 0.927517\tvalid_1's amex_metric: 0.799924\n",
            "[9000]\ttraining's amex_metric: 0.933445\tvalid_1's amex_metric: 0.799886\n",
            "[9500]\ttraining's amex_metric: 0.93871\tvalid_1's amex_metric: 0.799453\n",
            "Our Fold 4 CV score is 0.7999318158129617 \n",
            "Our out of folds CV Score is 0.7975196400549388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CV:0.7975"
      ],
      "metadata": {
        "id": "3Q5cpzxYIcRF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models=[]\n",
        "for i in range(5):\n",
        "  model=joblib.load(os.path.join(Config.MODEL,f'lgbm_fold{i}_seed{Config.seed}.pkl'))\n",
        "  models.append(model)"
      ],
      "metadata": {
        "id": "uvA9Q0nDDtIS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict the test set\n",
        "test=pd.read_parquet(os.path.join(Config.INPUT,'test_fe_plus_plus.parquet'))\n",
        "features=[col for col in test.columns if col not in ['customer_ID',Config.target]]\n",
        "test_predictions=np.zeros(len(test))\n",
        "for model in models:\n",
        "  test_pred=model.predict(test[features])\n",
        "  test_predictions+=test_pred/Config.n_folds\n",
        "#Create a dataframe to store test predictions\n",
        "test_df=pd.DataFrame({'customer_ID':test['customer_ID'],'prediction':test_predictions})\n",
        "test_df.to_csv(os.path.join(Config.PRED,f'test_lgbm_baseline_{Config.n_folds}fold_seed{Config.seed}.csv'),index=False)"
      ],
      "metadata": {
        "id": "35vu-EXOZhfg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LB: 0.787"
      ],
      "metadata": {
        "id": "X-Tzo6wxEy1t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}