{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exp018.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "mount_file_id": "1DOuFLT7zaEo_XZ3xb_FSpds0QNsay_xJ",
      "authorship_tag": "ABX9TyOVmUtE2LQXAZr7Fikd4hLR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DONDAJIN/Kaggle_amex/blob/master/exp018.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Df8QIDP5qFum"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import joblib\n",
        "import random\n",
        "import warnings\n",
        "import itertools\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import lightgbm as lgb\n",
        "from itertools import combinations\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings; warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split,KFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_difference(data, num_features):\n",
        "    df1 = []\n",
        "    customer_ids = []\n",
        "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
        "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
        "        df1.append(diff_df1)\n",
        "        customer_ids.append(customer_id)\n",
        "    df1 = np.concatenate(df1, axis = 0)\n",
        "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
        "    df1['customer_ID'] = customer_ids\n",
        "    return df1\n",
        "\n",
        "def read_preprocess_data():\n",
        "    train = pd.read_parquet(os.path.join(Config.INPUT,'train.parquet'))\n",
        "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
        "    cat_features = [\n",
        "        \"B_30\",\n",
        "        \"B_38\",\n",
        "        \"D_114\",\n",
        "        \"D_116\",\n",
        "        \"D_117\",\n",
        "        \"D_120\",\n",
        "        \"D_126\",\n",
        "        \"D_63\",\n",
        "        \"D_64\",\n",
        "        \"D_66\",\n",
        "        \"D_68\",\n",
        "    ]\n",
        "    num_features = [col for col in features if col not in cat_features]\n",
        "    print('Starting training feature engineer...')\n",
        "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
        "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
        "    train_num_agg.reset_index(inplace = True)\n",
        "\n",
        "    # Lag Features\n",
        "    for col in train_num_agg:\n",
        "        for col_2 in ['first', 'mean', 'std', 'min', 'max']:\n",
        "            if 'last' in col and col.replace('last', col_2) in train_num_agg:\n",
        "                train_num_agg[col + '_lag_sub'] = train_num_agg[col] - train_num_agg[col.replace('last', col_2)]\n",
        "                train_num_agg[col + '_lag_div'] = train_num_agg[col] / train_num_agg[col.replace('last', col_2)]\n",
        "\n",
        "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
        "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
        "    train_cat_agg.reset_index(inplace = True)\n",
        "    train_labels = pd.read_csv(os.path.join(Config.INPUT,'train_labels.csv'))\n",
        "    # Transform float64 columns to float32\n",
        "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
        "    for col in tqdm(cols):\n",
        "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
        "    # Transform int64 columns to int32\n",
        "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
        "    for col in tqdm(cols):\n",
        "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
        "    # Get the difference\n",
        "    train_diff = get_difference(train, num_features)\n",
        "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
        "    del train_num_agg, train_cat_agg, train_diff\n",
        "    gc.collect()\n",
        "    test = pd.read_parquet(os.path.join(Config.INPUT,'test.parquet'))\n",
        "    print('Starting test feature engineer...')\n",
        "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
        "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
        "    test_num_agg.reset_index(inplace = True)\n",
        "\n",
        "    # Lag Features\n",
        "    for col in test_num_agg:\n",
        "        for col_2 in ['first', 'mean', 'std', 'min', 'max']:\n",
        "            if 'last' in col and col.replace('last', col_2) in test_num_agg:\n",
        "                test_num_agg[col + '_lag_sub'] = test_num_agg[col] - test_num_agg[col.replace('last', col_2)]\n",
        "                test_num_agg[col + '_lag_div'] = test_num_agg[col] / test_num_agg[col.replace('last', col_2)]\n",
        "\n",
        "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
        "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
        "    test_cat_agg.reset_index(inplace = True)\n",
        "    # Transform float64 columns to float32\n",
        "    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
        "    for col in tqdm(cols):\n",
        "        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
        "    # Transform int64 columns to int32\n",
        "    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
        "    for col in tqdm(cols):\n",
        "        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
        "    # Get the difference\n",
        "    test_diff = get_difference(test, num_features)\n",
        "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
        "    del test_num_agg, test_cat_agg, test_diff\n",
        "    gc.collect()\n",
        "    # Save files to disk\n",
        "    train.to_parquet(os.path.join(Config.INPUT,'train_fe_v3_loaded.parquet'))\n",
        "    test.to_parquet(os.path.join(Config.INPUT,'test_fe_v3_loaded.parquet'))"
      ],
      "metadata": {
        "id": "u7BfZ-UMqKtr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    seed =92\n",
        "    n_folds = 5\n",
        "    target = 'target'\n",
        "    INPUT = '/content/drive/MyDrive/Kaggle/Input'\n",
        "    OUTPUT='/content/drive/MyDrive/Kaggle/Output'\n",
        "    EXP='/content/drive/MyDrive/Kaggle/exp/exp018'\n",
        "    MODEL=os.path.join(EXP,'Model')\n",
        "    LOG=os.path.join(EXP,'Log')\n",
        "    PRED=os.path.join(EXP,'pred')\n",
        "    os.makedirs(EXP,exist_ok=True)\n",
        "    for i in ['Log','Model','pred']:\n",
        "      os.makedirs(os.path.join(EXP,i),exist_ok=True)\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "def read_data():\n",
        "    train = pd.read_parquet(os.path.join(Config.INPUT,'train_fe_v3_loaded.parquet'))\n",
        "    test = pd.read_parquet(os.path.join(Config.INPUT,'test_fe_v3_loaded.parquet'))\n",
        "    return train, test\n",
        "\n",
        "def amex_metric(y_true, y_pred):\n",
        "    labels = np.transpose(np.array([y_true, y_pred]))\n",
        "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
        "    weights = np.where(labels[:,0]==0, 20, 1)\n",
        "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
        "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
        "    gini = [0,0]\n",
        "    for i in [1,0]:\n",
        "        labels = np.transpose(np.array([y_true, y_pred]))\n",
        "        labels = labels[labels[:, i].argsort()[::-1]]\n",
        "        weight = np.where(labels[:,0]==0, 20, 1)\n",
        "        weight_random = np.cumsum(weight / np.sum(weight))\n",
        "        total_pos = np.sum(labels[:, 0] *  weight)\n",
        "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
        "        lorentz = cum_pos_found / total_pos\n",
        "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
        "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
        "\n",
        "def amex_metric_np(preds, target):\n",
        "    indices = np.argsort(preds)[::-1]\n",
        "    preds, target = preds[indices], target[indices]\n",
        "    weight = 20.0 - target * 19.0\n",
        "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
        "    four_pct_mask = cum_norm_weight <= 0.04\n",
        "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
        "    weighted_target = target * weight\n",
        "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
        "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
        "    n_pos = np.sum(target)\n",
        "    n_neg = target.shape[0] - n_pos\n",
        "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
        "    g = gini / gini_max\n",
        "    return 0.5 * (g + d)"
      ],
      "metadata": {
        "id": "MruBWeBBq08-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lgb_amex_metric(y_pred, y_true):\n",
        "    y_true = y_true.get_label()\n",
        "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
        "\n",
        "def train_and_evaluate(train, test):\n",
        "    # Label encode categorical features\n",
        "    cat_features = [\n",
        "        \"B_30\",\n",
        "        \"B_38\",\n",
        "        \"D_114\",\n",
        "        \"D_116\",\n",
        "        \"D_117\",\n",
        "        \"D_120\",\n",
        "        \"D_126\",\n",
        "        \"D_63\",\n",
        "        \"D_64\",\n",
        "        \"D_66\",\n",
        "        \"D_68\"\n",
        "    ]\n",
        "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
        "    for cat_col in cat_features:\n",
        "        encoder = LabelEncoder()\n",
        "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
        "        test[cat_col] = encoder.transform(test[cat_col])\n",
        "    # Round last float features to 2 decimal place\n",
        "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
        "    num_cols = [col for col in num_cols if 'last' in col]\n",
        "    for col in num_cols:\n",
        "        train[col + '_round2'] = train[col].round(2)\n",
        "        test[col + '_round2'] = test[col].round(2)\n",
        "    # Get the difference between last and mean\n",
        "    num_cols = [col for col in train.columns if 'last' in col]\n",
        "    num_cols = [col[:-5] for col in num_cols if 'round' not in col]\n",
        "    for col in num_cols:\n",
        "        try:\n",
        "            train[f'{col}_last_mean_diff'] = train[f'{col}_last'] - train[f'{col}_mean']\n",
        "            test[f'{col}_last_mean_diff'] = test[f'{col}_last'] - test[f'{col}_mean']\n",
        "        except:\n",
        "            pass\n",
        "    # Transform float64 and float32 to float16\n",
        "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
        "    for col in tqdm(num_cols):\n",
        "        train[col] = train[col].astype(np.float16)\n",
        "        test[col] = test[col].astype(np.float16)\n",
        "    # Get feature list\n",
        "    features = [col for col in train.columns if col not in ['customer_ID', Config.target]]\n",
        "    features=[col for col in features if 'B_29' not in col ]\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': \"binary_logloss\",\n",
        "        'boosting': 'dart',\n",
        "        'seed': Config.seed,\n",
        "        'num_leaves': 100,\n",
        "        'learning_rate': 0.01,\n",
        "        'feature_fraction': 0.15,\n",
        "        'bagging_freq': 10,\n",
        "        'bagging_fraction': 0.50,\n",
        "        'n_jobs': -1,\n",
        "        'lambda_l2': 2.3,\n",
        "        'min_data_in_leaf': 40\n",
        "        }\n",
        "    # Create a numpy array to store test predictions\n",
        "    test_predictions = np.zeros(len(test))\n",
        "    # Create a numpy array to store out of folds predictions\n",
        "    oof_predictions = np.zeros(len(train))\n",
        "    kfold = StratifiedKFold(n_splits = Config.n_folds, shuffle = True, random_state = Config.seed)\n",
        "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[Config.target])):\n",
        "        print(' ')\n",
        "        print('-'*50)\n",
        "        print(f'Training fold {fold} with {len(features)} features...')\n",
        "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
        "        y_train, y_val = train[Config.target].iloc[trn_ind], train[Config.target].iloc[val_ind]\n",
        "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
        "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
        "        model = lgb.train(\n",
        "            params = params,\n",
        "            train_set = lgb_train,\n",
        "            num_boost_round = 10500,\n",
        "            valid_sets = [lgb_train, lgb_valid],\n",
        "            early_stopping_rounds = 500,\n",
        "            verbose_eval = 500,\n",
        "            feval = lgb_amex_metric\n",
        "            )\n",
        "        # Save best model\n",
        "        joblib.dump(model,os.path.join(Config.MODEL, f'lgbm_fold{fold}_seed{Config.seed}.pkl'))\n",
        "        \n",
        "        # Predict validation\n",
        "        val_pred = model.predict(x_val)\n",
        "        # Add to out of folds array\n",
        "        oof_predictions[val_ind] = val_pred\n",
        "        # Predict the test set\n",
        "        test_pred = model.predict(test[features])\n",
        "        test_predictions += test_pred / Config.n_folds\n",
        "        # Compute fold metric\n",
        "        score = amex_metric(y_val, val_pred)\n",
        "        print(f'Our fold {fold} CV score is {score}')\n",
        "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
        "        gc.collect()\n",
        "    # Compute out of folds metric\n",
        "    score = amex_metric(train[Config.target], oof_predictions)\n",
        "    print(f'Our out of folds CV score is {score}')\n",
        "    # Create a dataframe to store out of folds predictions\n",
        "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[Config.target], 'prediction': oof_predictions})\n",
        "    oof_df.to_csv(os.path.join(Config.PRED,f'oof_lgbm_baseline_{Config.n_folds}fold_seed{Config.seed}.csv'), index = False)\n",
        "    # Create a dataframe to store test prediction\n",
        "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
        "    test_df.to_csv(os.path.join(Config.PRED,f'test_lgbm_baseline_{Config.n_folds}fold_seed{Config.seed}.csv'), index = False)\n"
      ],
      "metadata": {
        "id": "WoqIiTkTsTJx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train,test=read_data()\n",
        "train_and_evaluate(train,test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpmQpFS3soxL",
        "outputId": "a956abab-1267-4620-fd7c-d99bd35d754b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1713/1713 [25:50<00:00,  1.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "--------------------------------------------------\n",
            "Training fold 0 with 2164 features...\n",
            "[500]\ttraining's binary_logloss: 0.31041\ttraining's amex_metric: 0.783359\tvalid_1's binary_logloss: 0.315168\tvalid_1's amex_metric: 0.764025\n",
            "[1000]\ttraining's binary_logloss: 0.25404\ttraining's amex_metric: 0.795328\tvalid_1's binary_logloss: 0.261807\tvalid_1's amex_metric: 0.772236\n",
            "[1500]\ttraining's binary_logloss: 0.223538\ttraining's amex_metric: 0.808317\tvalid_1's binary_logloss: 0.235538\tvalid_1's amex_metric: 0.778721\n",
            "[2000]\ttraining's binary_logloss: 0.209761\ttraining's amex_metric: 0.820748\tvalid_1's binary_logloss: 0.226533\tvalid_1's amex_metric: 0.783752\n",
            "[2500]\ttraining's binary_logloss: 0.201554\ttraining's amex_metric: 0.831462\tvalid_1's binary_logloss: 0.22291\tvalid_1's amex_metric: 0.785634\n",
            "[3000]\ttraining's binary_logloss: 0.194238\ttraining's amex_metric: 0.842523\tvalid_1's binary_logloss: 0.220497\tvalid_1's amex_metric: 0.787972\n",
            "[3500]\ttraining's binary_logloss: 0.188656\ttraining's amex_metric: 0.85211\tvalid_1's binary_logloss: 0.219341\tvalid_1's amex_metric: 0.789479\n",
            "[4000]\ttraining's binary_logloss: 0.182767\ttraining's amex_metric: 0.861333\tvalid_1's binary_logloss: 0.218232\tvalid_1's amex_metric: 0.790926\n",
            "[4500]\ttraining's binary_logloss: 0.177262\ttraining's amex_metric: 0.87035\tvalid_1's binary_logloss: 0.217537\tvalid_1's amex_metric: 0.790623\n",
            "[5000]\ttraining's binary_logloss: 0.171822\ttraining's amex_metric: 0.878926\tvalid_1's binary_logloss: 0.216901\tvalid_1's amex_metric: 0.791874\n",
            "[5500]\ttraining's binary_logloss: 0.16594\ttraining's amex_metric: 0.888453\tvalid_1's binary_logloss: 0.216389\tvalid_1's amex_metric: 0.792649\n",
            "[6000]\ttraining's binary_logloss: 0.161734\ttraining's amex_metric: 0.896304\tvalid_1's binary_logloss: 0.21618\tvalid_1's amex_metric: 0.792824\n",
            "[6500]\ttraining's binary_logloss: 0.156792\ttraining's amex_metric: 0.904571\tvalid_1's binary_logloss: 0.215882\tvalid_1's amex_metric: 0.793031\n",
            "[7000]\ttraining's binary_logloss: 0.152619\ttraining's amex_metric: 0.911609\tvalid_1's binary_logloss: 0.215767\tvalid_1's amex_metric: 0.792872\n",
            "[7500]\ttraining's binary_logloss: 0.148155\ttraining's amex_metric: 0.918683\tvalid_1's binary_logloss: 0.215619\tvalid_1's amex_metric: 0.792979\n",
            "[8000]\ttraining's binary_logloss: 0.144308\ttraining's amex_metric: 0.925559\tvalid_1's binary_logloss: 0.215514\tvalid_1's amex_metric: 0.792465\n",
            "[8500]\ttraining's binary_logloss: 0.140045\ttraining's amex_metric: 0.93144\tvalid_1's binary_logloss: 0.215425\tvalid_1's amex_metric: 0.792802\n",
            "[9000]\ttraining's binary_logloss: 0.136457\ttraining's amex_metric: 0.937472\tvalid_1's binary_logloss: 0.215382\tvalid_1's amex_metric: 0.792902\n",
            "[9500]\ttraining's binary_logloss: 0.132622\ttraining's amex_metric: 0.94268\tvalid_1's binary_logloss: 0.215328\tvalid_1's amex_metric: 0.793064\n",
            "[10000]\ttraining's binary_logloss: 0.128679\ttraining's amex_metric: 0.948559\tvalid_1's binary_logloss: 0.215263\tvalid_1's amex_metric: 0.793009\n",
            "[10500]\ttraining's binary_logloss: 0.125303\ttraining's amex_metric: 0.953415\tvalid_1's binary_logloss: 0.21525\tvalid_1's amex_metric: 0.792809\n",
            "Our fold 0 CV score is 0.7928089942172786\n",
            " \n",
            "--------------------------------------------------\n",
            "Training fold 1 with 2164 features...\n",
            "[500]\ttraining's binary_logloss: 0.310496\ttraining's amex_metric: 0.781531\tvalid_1's binary_logloss: 0.31497\tvalid_1's amex_metric: 0.77072\n",
            "[1000]\ttraining's binary_logloss: 0.254245\ttraining's amex_metric: 0.793674\tvalid_1's binary_logloss: 0.26155\tvalid_1's amex_metric: 0.779855\n",
            "[1500]\ttraining's binary_logloss: 0.223782\ttraining's amex_metric: 0.806803\tvalid_1's binary_logloss: 0.234901\tvalid_1's amex_metric: 0.78447\n",
            "[2000]\ttraining's binary_logloss: 0.210035\ttraining's amex_metric: 0.820024\tvalid_1's binary_logloss: 0.225582\tvalid_1's amex_metric: 0.790148\n",
            "[2500]\ttraining's binary_logloss: 0.20187\ttraining's amex_metric: 0.830622\tvalid_1's binary_logloss: 0.221797\tvalid_1's amex_metric: 0.793013\n",
            "[3000]\ttraining's binary_logloss: 0.194527\ttraining's amex_metric: 0.841035\tvalid_1's binary_logloss: 0.219233\tvalid_1's amex_metric: 0.79481\n",
            "[3500]\ttraining's binary_logloss: 0.188977\ttraining's amex_metric: 0.851143\tvalid_1's binary_logloss: 0.218069\tvalid_1's amex_metric: 0.795847\n",
            "[4000]\ttraining's binary_logloss: 0.183126\ttraining's amex_metric: 0.860521\tvalid_1's binary_logloss: 0.216932\tvalid_1's amex_metric: 0.797089\n",
            "[4500]\ttraining's binary_logloss: 0.177602\ttraining's amex_metric: 0.86937\tvalid_1's binary_logloss: 0.216196\tvalid_1's amex_metric: 0.797579\n",
            "[5000]\ttraining's binary_logloss: 0.17218\ttraining's amex_metric: 0.87794\tvalid_1's binary_logloss: 0.215621\tvalid_1's amex_metric: 0.79781\n",
            "[5500]\ttraining's binary_logloss: 0.166314\ttraining's amex_metric: 0.887816\tvalid_1's binary_logloss: 0.215126\tvalid_1's amex_metric: 0.798229\n",
            "[6000]\ttraining's binary_logloss: 0.162132\ttraining's amex_metric: 0.895947\tvalid_1's binary_logloss: 0.214953\tvalid_1's amex_metric: 0.798493\n",
            "[6500]\ttraining's binary_logloss: 0.157194\ttraining's amex_metric: 0.904153\tvalid_1's binary_logloss: 0.214702\tvalid_1's amex_metric: 0.798945\n",
            "[7000]\ttraining's binary_logloss: 0.153001\ttraining's amex_metric: 0.911289\tvalid_1's binary_logloss: 0.21454\tvalid_1's amex_metric: 0.799278\n",
            "[7500]\ttraining's binary_logloss: 0.148532\ttraining's amex_metric: 0.918659\tvalid_1's binary_logloss: 0.214308\tvalid_1's amex_metric: 0.799148\n",
            "[8000]\ttraining's binary_logloss: 0.144693\ttraining's amex_metric: 0.92495\tvalid_1's binary_logloss: 0.214267\tvalid_1's amex_metric: 0.799893\n",
            "[8500]\ttraining's binary_logloss: 0.140425\ttraining's amex_metric: 0.931608\tvalid_1's binary_logloss: 0.214157\tvalid_1's amex_metric: 0.800314\n",
            "[9000]\ttraining's binary_logloss: 0.136807\ttraining's amex_metric: 0.937566\tvalid_1's binary_logloss: 0.214099\tvalid_1's amex_metric: 0.800058\n",
            "[9500]\ttraining's binary_logloss: 0.132976\ttraining's amex_metric: 0.942784\tvalid_1's binary_logloss: 0.214095\tvalid_1's amex_metric: 0.800154\n",
            "[10000]\ttraining's binary_logloss: 0.129023\ttraining's amex_metric: 0.948423\tvalid_1's binary_logloss: 0.214016\tvalid_1's amex_metric: 0.800114\n",
            "[10500]\ttraining's binary_logloss: 0.125652\ttraining's amex_metric: 0.953445\tvalid_1's binary_logloss: 0.213975\tvalid_1's amex_metric: 0.8004\n",
            "Our fold 1 CV score is 0.8003996896312807\n",
            " \n",
            "--------------------------------------------------\n",
            "Training fold 2 with 2164 features...\n",
            "[500]\ttraining's binary_logloss: 0.310942\ttraining's amex_metric: 0.781648\tvalid_1's binary_logloss: 0.313422\tvalid_1's amex_metric: 0.770068\n",
            "[1000]\ttraining's binary_logloss: 0.254769\ttraining's amex_metric: 0.793835\tvalid_1's binary_logloss: 0.259814\tvalid_1's amex_metric: 0.778087\n",
            "[1500]\ttraining's binary_logloss: 0.224183\ttraining's amex_metric: 0.807363\tvalid_1's binary_logloss: 0.233165\tvalid_1's amex_metric: 0.786028\n",
            "[2000]\ttraining's binary_logloss: 0.210327\ttraining's amex_metric: 0.819933\tvalid_1's binary_logloss: 0.224017\tvalid_1's amex_metric: 0.790454\n",
            "[2500]\ttraining's binary_logloss: 0.202099\ttraining's amex_metric: 0.830168\tvalid_1's binary_logloss: 0.220396\tvalid_1's amex_metric: 0.791236\n",
            "[3000]\ttraining's binary_logloss: 0.194731\ttraining's amex_metric: 0.841496\tvalid_1's binary_logloss: 0.218053\tvalid_1's amex_metric: 0.793469\n",
            "[3500]\ttraining's binary_logloss: 0.189135\ttraining's amex_metric: 0.851343\tvalid_1's binary_logloss: 0.216994\tvalid_1's amex_metric: 0.794837\n",
            "[4000]\ttraining's binary_logloss: 0.183273\ttraining's amex_metric: 0.860359\tvalid_1's binary_logloss: 0.215946\tvalid_1's amex_metric: 0.79554\n",
            "[4500]\ttraining's binary_logloss: 0.1778\ttraining's amex_metric: 0.86946\tvalid_1's binary_logloss: 0.215209\tvalid_1's amex_metric: 0.79676\n",
            "[5000]\ttraining's binary_logloss: 0.172356\ttraining's amex_metric: 0.87823\tvalid_1's binary_logloss: 0.214579\tvalid_1's amex_metric: 0.797615\n",
            "[5500]\ttraining's binary_logloss: 0.166498\ttraining's amex_metric: 0.888112\tvalid_1's binary_logloss: 0.214079\tvalid_1's amex_metric: 0.798621\n",
            "[6000]\ttraining's binary_logloss: 0.162287\ttraining's amex_metric: 0.895538\tvalid_1's binary_logloss: 0.213933\tvalid_1's amex_metric: 0.798946\n",
            "[6500]\ttraining's binary_logloss: 0.15735\ttraining's amex_metric: 0.90364\tvalid_1's binary_logloss: 0.213717\tvalid_1's amex_metric: 0.798988\n",
            "[7000]\ttraining's binary_logloss: 0.153184\ttraining's amex_metric: 0.910999\tvalid_1's binary_logloss: 0.213585\tvalid_1's amex_metric: 0.7985\n",
            "[7500]\ttraining's binary_logloss: 0.148707\ttraining's amex_metric: 0.918437\tvalid_1's binary_logloss: 0.213386\tvalid_1's amex_metric: 0.798482\n",
            "[8000]\ttraining's binary_logloss: 0.144891\ttraining's amex_metric: 0.925167\tvalid_1's binary_logloss: 0.213269\tvalid_1's amex_metric: 0.799026\n",
            "[8500]\ttraining's binary_logloss: 0.140649\ttraining's amex_metric: 0.931274\tvalid_1's binary_logloss: 0.213226\tvalid_1's amex_metric: 0.798829\n",
            "[9000]\ttraining's binary_logloss: 0.137048\ttraining's amex_metric: 0.937398\tvalid_1's binary_logloss: 0.213187\tvalid_1's amex_metric: 0.799344\n",
            "[9500]\ttraining's binary_logloss: 0.133232\ttraining's amex_metric: 0.942727\tvalid_1's binary_logloss: 0.213134\tvalid_1's amex_metric: 0.799438\n",
            "[10000]\ttraining's binary_logloss: 0.129299\ttraining's amex_metric: 0.948736\tvalid_1's binary_logloss: 0.21306\tvalid_1's amex_metric: 0.79929\n",
            "[10500]\ttraining's binary_logloss: 0.125919\ttraining's amex_metric: 0.953362\tvalid_1's binary_logloss: 0.212983\tvalid_1's amex_metric: 0.799296\n",
            "Our fold 2 CV score is 0.7992963995358704\n",
            " \n",
            "--------------------------------------------------\n",
            "Training fold 3 with 2164 features...\n",
            "[500]\ttraining's binary_logloss: 0.310519\ttraining's amex_metric: 0.781858\tvalid_1's binary_logloss: 0.314463\tvalid_1's amex_metric: 0.7664\n",
            "[1000]\ttraining's binary_logloss: 0.254223\ttraining's amex_metric: 0.794454\tvalid_1's binary_logloss: 0.261092\tvalid_1's amex_metric: 0.775419\n",
            "[1500]\ttraining's binary_logloss: 0.223727\ttraining's amex_metric: 0.807432\tvalid_1's binary_logloss: 0.234669\tvalid_1's amex_metric: 0.781695\n",
            "[2000]\ttraining's binary_logloss: 0.209915\ttraining's amex_metric: 0.820178\tvalid_1's binary_logloss: 0.225648\tvalid_1's amex_metric: 0.786624\n",
            "[2500]\ttraining's binary_logloss: 0.201692\ttraining's amex_metric: 0.831085\tvalid_1's binary_logloss: 0.222055\tvalid_1's amex_metric: 0.78985\n",
            "[3000]\ttraining's binary_logloss: 0.194353\ttraining's amex_metric: 0.841374\tvalid_1's binary_logloss: 0.21958\tvalid_1's amex_metric: 0.792143\n",
            "[3500]\ttraining's binary_logloss: 0.188798\ttraining's amex_metric: 0.850933\tvalid_1's binary_logloss: 0.218406\tvalid_1's amex_metric: 0.792852\n",
            "[4000]\ttraining's binary_logloss: 0.182952\ttraining's amex_metric: 0.859989\tvalid_1's binary_logloss: 0.217359\tvalid_1's amex_metric: 0.794613\n",
            "[4500]\ttraining's binary_logloss: 0.177448\ttraining's amex_metric: 0.869958\tvalid_1's binary_logloss: 0.216607\tvalid_1's amex_metric: 0.794225\n",
            "[5000]\ttraining's binary_logloss: 0.17203\ttraining's amex_metric: 0.878487\tvalid_1's binary_logloss: 0.215958\tvalid_1's amex_metric: 0.794736\n",
            "[5500]\ttraining's binary_logloss: 0.166175\ttraining's amex_metric: 0.888259\tvalid_1's binary_logloss: 0.215494\tvalid_1's amex_metric: 0.795952\n",
            "[6000]\ttraining's binary_logloss: 0.161958\ttraining's amex_metric: 0.89595\tvalid_1's binary_logloss: 0.215292\tvalid_1's amex_metric: 0.795792\n",
            "[6500]\ttraining's binary_logloss: 0.157038\ttraining's amex_metric: 0.904423\tvalid_1's binary_logloss: 0.215023\tvalid_1's amex_metric: 0.796284\n",
            "[7000]\ttraining's binary_logloss: 0.152869\ttraining's amex_metric: 0.911471\tvalid_1's binary_logloss: 0.214936\tvalid_1's amex_metric: 0.796052\n",
            "[7500]\ttraining's binary_logloss: 0.148409\ttraining's amex_metric: 0.918384\tvalid_1's binary_logloss: 0.214767\tvalid_1's amex_metric: 0.796746\n",
            "[8000]\ttraining's binary_logloss: 0.144569\ttraining's amex_metric: 0.925184\tvalid_1's binary_logloss: 0.214709\tvalid_1's amex_metric: 0.796362\n",
            "[8500]\ttraining's binary_logloss: 0.14032\ttraining's amex_metric: 0.931881\tvalid_1's binary_logloss: 0.214581\tvalid_1's amex_metric: 0.796817\n",
            "[9000]\ttraining's binary_logloss: 0.136702\ttraining's amex_metric: 0.937751\tvalid_1's binary_logloss: 0.214511\tvalid_1's amex_metric: 0.797346\n",
            "[9500]\ttraining's binary_logloss: 0.13286\ttraining's amex_metric: 0.943018\tvalid_1's binary_logloss: 0.214403\tvalid_1's amex_metric: 0.797843\n",
            "[10000]\ttraining's binary_logloss: 0.12891\ttraining's amex_metric: 0.948835\tvalid_1's binary_logloss: 0.214366\tvalid_1's amex_metric: 0.797604\n",
            "[10500]\ttraining's binary_logloss: 0.125539\ttraining's amex_metric: 0.953665\tvalid_1's binary_logloss: 0.214326\tvalid_1's amex_metric: 0.797322\n",
            "Our fold 3 CV score is 0.7973221890644845\n",
            " \n",
            "--------------------------------------------------\n",
            "Training fold 4 with 2164 features...\n",
            "[500]\ttraining's binary_logloss: 0.310475\ttraining's amex_metric: 0.781572\tvalid_1's binary_logloss: 0.314694\tvalid_1's amex_metric: 0.769341\n",
            "[1000]\ttraining's binary_logloss: 0.254296\ttraining's amex_metric: 0.793749\tvalid_1's binary_logloss: 0.261215\tvalid_1's amex_metric: 0.77641\n",
            "[1500]\ttraining's binary_logloss: 0.223785\ttraining's amex_metric: 0.807201\tvalid_1's binary_logloss: 0.234702\tvalid_1's amex_metric: 0.783107\n",
            "[2000]\ttraining's binary_logloss: 0.210028\ttraining's amex_metric: 0.819871\tvalid_1's binary_logloss: 0.225543\tvalid_1's amex_metric: 0.78706\n",
            "[2500]\ttraining's binary_logloss: 0.201833\ttraining's amex_metric: 0.830632\tvalid_1's binary_logloss: 0.22182\tvalid_1's amex_metric: 0.789862\n",
            "[3000]\ttraining's binary_logloss: 0.194487\ttraining's amex_metric: 0.841011\tvalid_1's binary_logloss: 0.219415\tvalid_1's amex_metric: 0.791461\n",
            "[3500]\ttraining's binary_logloss: 0.188899\ttraining's amex_metric: 0.851157\tvalid_1's binary_logloss: 0.218263\tvalid_1's amex_metric: 0.793114\n",
            "[4000]\ttraining's binary_logloss: 0.18307\ttraining's amex_metric: 0.859941\tvalid_1's binary_logloss: 0.217238\tvalid_1's amex_metric: 0.793899\n",
            "[4500]\ttraining's binary_logloss: 0.177575\ttraining's amex_metric: 0.869345\tvalid_1's binary_logloss: 0.216518\tvalid_1's amex_metric: 0.794739\n",
            "[5000]\ttraining's binary_logloss: 0.1721\ttraining's amex_metric: 0.878661\tvalid_1's binary_logloss: 0.215825\tvalid_1's amex_metric: 0.795348\n",
            "[5500]\ttraining's binary_logloss: 0.16622\ttraining's amex_metric: 0.88829\tvalid_1's binary_logloss: 0.215304\tvalid_1's amex_metric: 0.79526\n",
            "[6000]\ttraining's binary_logloss: 0.161982\ttraining's amex_metric: 0.89614\tvalid_1's binary_logloss: 0.215124\tvalid_1's amex_metric: 0.795931\n",
            "[6500]\ttraining's binary_logloss: 0.157058\ttraining's amex_metric: 0.904044\tvalid_1's binary_logloss: 0.214874\tvalid_1's amex_metric: 0.796959\n",
            "[7000]\ttraining's binary_logloss: 0.152899\ttraining's amex_metric: 0.911649\tvalid_1's binary_logloss: 0.214715\tvalid_1's amex_metric: 0.796817\n",
            "[7500]\ttraining's binary_logloss: 0.148443\ttraining's amex_metric: 0.918334\tvalid_1's binary_logloss: 0.214628\tvalid_1's amex_metric: 0.796668\n",
            "[8000]\ttraining's binary_logloss: 0.144599\ttraining's amex_metric: 0.925434\tvalid_1's binary_logloss: 0.214521\tvalid_1's amex_metric: 0.796813\n",
            "[8500]\ttraining's binary_logloss: 0.140346\ttraining's amex_metric: 0.931565\tvalid_1's binary_logloss: 0.214391\tvalid_1's amex_metric: 0.79646\n",
            "[9000]\ttraining's binary_logloss: 0.136713\ttraining's amex_metric: 0.937543\tvalid_1's binary_logloss: 0.214289\tvalid_1's amex_metric: 0.797147\n",
            "[9500]\ttraining's binary_logloss: 0.132909\ttraining's amex_metric: 0.943282\tvalid_1's binary_logloss: 0.214239\tvalid_1's amex_metric: 0.797178\n",
            "[10000]\ttraining's binary_logloss: 0.128974\ttraining's amex_metric: 0.948641\tvalid_1's binary_logloss: 0.214176\tvalid_1's amex_metric: 0.797266\n",
            "[10500]\ttraining's binary_logloss: 0.125611\ttraining's amex_metric: 0.953624\tvalid_1's binary_logloss: 0.214174\tvalid_1's amex_metric: 0.796972\n",
            "Our fold 4 CV score is 0.7969720597684173\n",
            "Our out of folds CV score is 0.7974417132114643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CV: 0.7974417132114643"
      ],
      "metadata": {
        "id": "3CB7utflVORK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xfxAXfMbVP1n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}