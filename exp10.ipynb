{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "mount_file_id": "1_6pZmuaSWDDuUdSovOpQuAoeRK5-pxqX",
      "authorship_tag": "ABX9TyNSn1l/QRDv3u0gicEvMcgt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DONDAJIN/Kaggle_amex/blob/master/exp10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "K_1AHt1YGtlK"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import joblib\n",
        "import random\n",
        "import warnings\n",
        "import itertools\n",
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import lightgbm as lgb\n",
        "from itertools import combinations\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings; warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "\n",
        "def get_difference(data, num_features):\n",
        "    df1 = []\n",
        "    customer_ids = []\n",
        "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
        "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
        "        df1.append(diff_df1)\n",
        "        customer_ids.append(customer_id)\n",
        "    df1 = np.concatenate(df1, axis = 0)\n",
        "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
        "    df1['customer_ID'] = customer_ids\n",
        "    return df1\n",
        "\n",
        "def read_preprocess_data():\n",
        "    train = pd.read_parquet(os.path.join(Config.INPUT,'train.parquet'))\n",
        "    features = train.drop(['customer_ID', 'S_2'], axis = 1).columns.to_list()\n",
        "    cat_features = [\n",
        "        \"B_30\",\n",
        "        \"B_38\",\n",
        "        \"D_114\",\n",
        "        \"D_116\",\n",
        "        \"D_117\",\n",
        "        \"D_120\",\n",
        "        \"D_126\",\n",
        "        \"D_63\",\n",
        "        \"D_64\",\n",
        "        \"D_66\",\n",
        "        \"D_68\",\n",
        "    ]\n",
        "    num_features = [col for col in features if col not in cat_features]\n",
        "    print('Starting training feature engineer...')\n",
        "    train_num_agg = train.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
        "    train_num_agg.columns = ['_'.join(x) for x in train_num_agg.columns]\n",
        "    train_num_agg.reset_index(inplace = True)\n",
        "\n",
        "    # Lag Features\n",
        "    for col in train_num_agg:\n",
        "        for col_2 in ['first', 'mean', 'std', 'min', 'max']:\n",
        "            if 'last' in col and col.replace('last', col_2) in train_num_agg:\n",
        "                train_num_agg[col + '_lag_sub'] = train_num_agg[col] - train_num_agg[col.replace('last', col_2)]\n",
        "                train_num_agg[col + '_lag_div'] = train_num_agg[col] / train_num_agg[col.replace('last', col_2)]\n",
        "\n",
        "    train_cat_agg = train.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
        "    train_cat_agg.columns = ['_'.join(x) for x in train_cat_agg.columns]\n",
        "    train_cat_agg.reset_index(inplace = True)\n",
        "    train_labels = pd.read_csv(os.path.join(Config.INPUT,'train_labels.csv'))\n",
        "    # Transform float64 columns to float32\n",
        "    cols = list(train_num_agg.dtypes[train_num_agg.dtypes == 'float64'].index)\n",
        "    for col in tqdm(cols):\n",
        "        train_num_agg[col] = train_num_agg[col].astype(np.float32)\n",
        "    # Transform int64 columns to int32\n",
        "    cols = list(train_cat_agg.dtypes[train_cat_agg.dtypes == 'int64'].index)\n",
        "    for col in tqdm(cols):\n",
        "        train_cat_agg[col] = train_cat_agg[col].astype(np.int32)\n",
        "    # Get the difference\n",
        "    train_diff = get_difference(train, num_features)\n",
        "    train = train_num_agg.merge(train_cat_agg, how = 'inner', on = 'customer_ID').merge(train_diff, how = 'inner', on = 'customer_ID').merge(train_labels, how = 'inner', on = 'customer_ID')\n",
        "    del train_num_agg, train_cat_agg, train_diff\n",
        "    gc.collect()\n",
        "    test = pd.read_parquet(os.path.join(Config.INPUT,'test.parquet'))\n",
        "    print('Starting test feature engineer...')\n",
        "    test_num_agg = test.groupby(\"customer_ID\")[num_features].agg(['first', 'mean', 'std', 'min', 'max', 'last'])\n",
        "    test_num_agg.columns = ['_'.join(x) for x in test_num_agg.columns]\n",
        "    test_num_agg.reset_index(inplace = True)\n",
        "\n",
        "    # Lag Features\n",
        "    for col in test_num_agg:\n",
        "        for col_2 in ['first', 'mean', 'std', 'min', 'max']:\n",
        "            if 'last' in col and col.replace('last', col_2) in test_num_agg:\n",
        "                test_num_agg[col + '_lag_sub'] = test_num_agg[col] - test_num_agg[col.replace('last', col_2)]\n",
        "                test_num_agg[col + '_lag_div'] = test_num_agg[col] / test_num_agg[col.replace('last', col_2)]\n",
        "\n",
        "    test_cat_agg = test.groupby(\"customer_ID\")[cat_features].agg(['count', 'first', 'last', 'nunique'])\n",
        "    test_cat_agg.columns = ['_'.join(x) for x in test_cat_agg.columns]\n",
        "    test_cat_agg.reset_index(inplace = True)\n",
        "    # Transform float64 columns to float32\n",
        "    cols = list(test_num_agg.dtypes[test_num_agg.dtypes == 'float64'].index)\n",
        "    for col in tqdm(cols):\n",
        "        test_num_agg[col] = test_num_agg[col].astype(np.float32)\n",
        "    # Transform int64 columns to int32\n",
        "    cols = list(test_cat_agg.dtypes[test_cat_agg.dtypes == 'int64'].index)\n",
        "    for col in tqdm(cols):\n",
        "        test_cat_agg[col] = test_cat_agg[col].astype(np.int32)\n",
        "    # Get the difference\n",
        "    test_diff = get_difference(test, num_features)\n",
        "    test = test_num_agg.merge(test_cat_agg, how = 'inner', on = 'customer_ID').merge(test_diff, how = 'inner', on = 'customer_ID')\n",
        "    del test_num_agg, test_cat_agg, test_diff\n",
        "    gc.collect()\n",
        "    # Save files to disk\n",
        "    train.to_parquet(os.path.join(Config.INPUT,'train_fe_v3_loaded.parquet'))\n",
        "    test.to_parquet(os.path.join(Config.INPUT,'test_fe_v3_loaded.parquet'))\n",
        "    \n",
        "# Read & Preprocess Data\n",
        "# read_preprocess_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    seed = 42\n",
        "    n_folds = 5\n",
        "    target = 'target'\n",
        "    INPUT = '/content/drive/MyDrive/Kaggle/Input'\n",
        "    OUTPUT='/content/drive/MyDrive/Kaggle/Output'\n",
        "    EXP='/content/drive/MyDrive/Kaggle/exp/exp010'\n",
        "    MODEL=os.path.join(EXP,'Model')\n",
        "    LOG=os.path.join(EXP,'Log')\n",
        "    PRED=os.path.join(EXP,'pred')\n",
        "    os.makedirs(EXP,exist_ok=True)\n",
        "    for i in ['Log','Model','pred']:\n",
        "      os.makedirs(os.path.join(EXP,i),exist_ok=True)\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "def read_data():\n",
        "    train = pd.read_parquet(os.path.join(Config.INPUT,'train_fe_v3_loaded.parquet'))\n",
        "    test = pd.read_parquet(os.path.join(Config.INPUT,'test_fe_v3_loaded.parquet'))\n",
        "    return train, test\n",
        "\n",
        "def amex_metric(y_true, y_pred):\n",
        "    labels = np.transpose(np.array([y_true, y_pred]))\n",
        "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
        "    weights = np.where(labels[:,0]==0, 20, 1)\n",
        "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
        "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
        "    gini = [0,0]\n",
        "    for i in [1,0]:\n",
        "        labels = np.transpose(np.array([y_true, y_pred]))\n",
        "        labels = labels[labels[:, i].argsort()[::-1]]\n",
        "        weight = np.where(labels[:,0]==0, 20, 1)\n",
        "        weight_random = np.cumsum(weight / np.sum(weight))\n",
        "        total_pos = np.sum(labels[:, 0] *  weight)\n",
        "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
        "        lorentz = cum_pos_found / total_pos\n",
        "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
        "    return 0.5 * (gini[1]/gini[0] + top_four)\n",
        "\n",
        "def amex_metric_np(preds, target):\n",
        "    indices = np.argsort(preds)[::-1]\n",
        "    preds, target = preds[indices], target[indices]\n",
        "    weight = 20.0 - target * 19.0\n",
        "    cum_norm_weight = (weight / weight.sum()).cumsum()\n",
        "    four_pct_mask = cum_norm_weight <= 0.04\n",
        "    d = np.sum(target[four_pct_mask]) / np.sum(target)\n",
        "    weighted_target = target * weight\n",
        "    lorentz = (weighted_target / weighted_target.sum()).cumsum()\n",
        "    gini = ((lorentz - cum_norm_weight) * weight).sum()\n",
        "    n_pos = np.sum(target)\n",
        "    n_neg = target.shape[0] - n_pos\n",
        "    gini_max = 10 * n_neg * (n_pos + 20 * n_neg - 19) / (n_pos + 20 * n_neg)\n",
        "    g = gini / gini_max\n",
        "    return 0.5 * (g + d)"
      ],
      "metadata": {
        "id": "pmKT7iv5JndM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lgb_amex_metric(y_pred, y_true):\n",
        "    y_true = y_true.get_label()\n",
        "    return 'amex_metric', amex_metric(y_true, y_pred), True\n",
        "\n",
        "def train_and_evaluate(train, test):\n",
        "    # Label encode categorical features\n",
        "    cat_features = [\n",
        "        \"B_30\",\n",
        "        \"B_38\",\n",
        "        \"D_114\",\n",
        "        \"D_116\",\n",
        "        \"D_117\",\n",
        "        \"D_120\",\n",
        "        \"D_126\",\n",
        "        \"D_63\",\n",
        "        \"D_64\",\n",
        "        \"D_66\",\n",
        "        \"D_68\"\n",
        "    ]\n",
        "    cat_features = [f\"{cf}_last\" for cf in cat_features]\n",
        "    for cat_col in cat_features:\n",
        "        encoder = LabelEncoder()\n",
        "        train[cat_col] = encoder.fit_transform(train[cat_col])\n",
        "        test[cat_col] = encoder.transform(test[cat_col])\n",
        "    # Round last float features to 2 decimal place\n",
        "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
        "    num_cols = [col for col in num_cols if 'last' in col]\n",
        "    for col in num_cols:\n",
        "        train[col + '_round2'] = train[col].round(2)\n",
        "        test[col + '_round2'] = test[col].round(2)\n",
        "    # Get the difference between last and mean\n",
        "    num_cols = [col for col in train.columns if 'last' in col]\n",
        "    num_cols = [col[:-5] for col in num_cols if 'round' not in col]\n",
        "    for col in num_cols:\n",
        "        try:\n",
        "            train[f'{col}_last_mean_diff'] = train[f'{col}_last'] - train[f'{col}_mean']\n",
        "            test[f'{col}_last_mean_diff'] = test[f'{col}_last'] - test[f'{col}_mean']\n",
        "        except:\n",
        "            pass\n",
        "    # Transform float64 and float32 to float16\n",
        "    num_cols = list(train.dtypes[(train.dtypes == 'float32') | (train.dtypes == 'float64')].index)\n",
        "    for col in tqdm(num_cols):\n",
        "        train[col] = train[col].astype(np.float16)\n",
        "        test[col] = test[col].astype(np.float16)\n",
        "    # Get feature list\n",
        "    features = [col for col in train.columns if col not in ['customer_ID', Config.target]]\n",
        "    params = {\n",
        "        'objective': 'binary',\n",
        "        'metric': \"binary_logloss\",\n",
        "        'boosting': 'gbdt',\n",
        "        'seed': Config.seed,\n",
        "        'num_leaves': 100,\n",
        "        'learning_rate': 0.01,\n",
        "        'feature_fraction': 0.20,\n",
        "        'bagging_freq': 10,\n",
        "        'bagging_fraction': 0.50,\n",
        "        'n_jobs': -1,\n",
        "        'lambda_l2': 2.3,\n",
        "        'min_data_in_leaf': 40\n",
        "        }\n",
        "    # Create a numpy array to store test predictions\n",
        "    test_predictions = np.zeros(len(test))\n",
        "    # Create a numpy array to store out of folds predictions\n",
        "    oof_predictions = np.zeros(len(train))\n",
        "    kfold = StratifiedKFold(n_splits = Config.n_folds, shuffle = True, random_state = Config.seed)\n",
        "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train, train[Config.target])):\n",
        "        print(' ')\n",
        "        print('-'*50)\n",
        "        print(f'Training fold {fold} with {len(features)} features...')\n",
        "        x_train, x_val = train[features].iloc[trn_ind], train[features].iloc[val_ind]\n",
        "        y_train, y_val = train[Config.target].iloc[trn_ind], train[Config.target].iloc[val_ind]\n",
        "        lgb_train = lgb.Dataset(x_train, y_train, categorical_feature = cat_features)\n",
        "        lgb_valid = lgb.Dataset(x_val, y_val, categorical_feature = cat_features)\n",
        "        model = lgb.train(\n",
        "            params = params,\n",
        "            train_set = lgb_train,\n",
        "            num_boost_round = 10500,\n",
        "            valid_sets = [lgb_train, lgb_valid],\n",
        "            early_stopping_rounds = 500,\n",
        "            verbose_eval = 500,\n",
        "            feval = lgb_amex_metric\n",
        "            )\n",
        "        # Save best model\n",
        "        joblib.dump(model,os.path.join(Config.MODEL, f'lgbm_fold{fold}_seed{Config.seed}.pkl'))\n",
        "        # Predict validation\n",
        "        val_pred = model.predict(x_val)\n",
        "        # Add to out of folds array\n",
        "        oof_predictions[val_ind] = val_pred\n",
        "        # Predict the test set\n",
        "        test_pred = model.predict(test[features])\n",
        "        test_predictions += test_pred / Config.n_folds\n",
        "        # Compute fold metric\n",
        "        score = amex_metric(y_val, val_pred)\n",
        "        print(f'Our fold {fold} CV score is {score}')\n",
        "        del x_train, x_val, y_train, y_val, lgb_train, lgb_valid\n",
        "        gc.collect()\n",
        "    # Compute out of folds metric\n",
        "    score = amex_metric(train[Config.target], oof_predictions)\n",
        "    print(f'Our out of folds CV score is {score}')\n",
        "    # Create a dataframe to store out of folds predictions\n",
        "    oof_df = pd.DataFrame({'customer_ID': train['customer_ID'], 'target': train[Config.target], 'prediction': oof_predictions})\n",
        "    oof_df.to_csv(os.path.join(Config.PRED,f'oof_lgbm_baseline_{Config.n_folds}fold_seed{Config.seed}.csv'), index = False)\n",
        "    # Create a dataframe to store test prediction\n",
        "    test_df = pd.DataFrame({'customer_ID': test['customer_ID'], 'prediction': test_predictions})\n",
        "    test_df.to_csv(os.path.join(Config.PRED,f'test_lgbm_baseline_{Config.n_folds}fold_seed{Config.seed}.csv'), index = False)\n",
        "\n"
      ],
      "metadata": {
        "id": "Dtl-K3M4JtdQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(Config.seed)\n",
        "read_preprocess_data()\n",
        "# train, test = read_data()\n",
        "# train_and_evaluate(train, test)"
      ],
      "metadata": {
        "id": "Z-Qs5JBRK3oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train,test=read_data()\n",
        "train_and_evaluate(train,test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqfsKpuRL6Os",
        "outputId": "504c361c-5a54-4df4-9d1d-7d145a73f269"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1713/1713 [15:19<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "--------------------------------------------------\n",
            "Training fold 0 with 2177 features...\n",
            "Training until validation scores don't improve for 500 rounds.\n",
            "[500]\ttraining's binary_logloss: 0.210124\ttraining's amex_metric: 0.810646\tvalid_1's binary_logloss: 0.222138\tvalid_1's amex_metric: 0.791229\n",
            "[1000]\ttraining's binary_logloss: 0.188783\ttraining's amex_metric: 0.842382\tvalid_1's binary_logloss: 0.215737\tvalid_1's amex_metric: 0.797609\n",
            "[1500]\ttraining's binary_logloss: 0.173203\ttraining's amex_metric: 0.86893\tvalid_1's binary_logloss: 0.21419\tvalid_1's amex_metric: 0.8\n",
            "[2000]\ttraining's binary_logloss: 0.159764\ttraining's amex_metric: 0.891811\tvalid_1's binary_logloss: 0.213475\tvalid_1's amex_metric: 0.801188\n",
            "[2500]\ttraining's binary_logloss: 0.14783\ttraining's amex_metric: 0.912305\tvalid_1's binary_logloss: 0.213187\tvalid_1's amex_metric: 0.800888\n",
            "Early stopping, best iteration is:\n",
            "[2032]\ttraining's binary_logloss: 0.158957\ttraining's amex_metric: 0.893161\tvalid_1's binary_logloss: 0.213436\tvalid_1's amex_metric: 0.80156\n",
            "Our fold 0 CV score is 0.8015598805532209\n",
            " \n",
            "--------------------------------------------------\n",
            "Training fold 1 with 2177 features...\n",
            "Training until validation scores don't improve for 500 rounds.\n",
            "[500]\ttraining's binary_logloss: 0.2093\ttraining's amex_metric: 0.81272\tvalid_1's binary_logloss: 0.224666\tvalid_1's amex_metric: 0.783557\n",
            "[1000]\ttraining's binary_logloss: 0.187882\ttraining's amex_metric: 0.84421\tvalid_1's binary_logloss: 0.218743\tvalid_1's amex_metric: 0.789723\n",
            "[1500]\ttraining's binary_logloss: 0.172281\ttraining's amex_metric: 0.870174\tvalid_1's binary_logloss: 0.217336\tvalid_1's amex_metric: 0.791718\n",
            "[2000]\ttraining's binary_logloss: 0.158903\ttraining's amex_metric: 0.893353\tvalid_1's binary_logloss: 0.216844\tvalid_1's amex_metric: 0.792888\n",
            "[2500]\ttraining's binary_logloss: 0.14697\ttraining's amex_metric: 0.913432\tvalid_1's binary_logloss: 0.216572\tvalid_1's amex_metric: 0.793574\n",
            "Early stopping, best iteration is:\n",
            "[2068]\ttraining's binary_logloss: 0.157213\ttraining's amex_metric: 0.896017\tvalid_1's binary_logloss: 0.216811\tvalid_1's amex_metric: 0.79406\n",
            "Our fold 1 CV score is 0.7940597262832256\n",
            " \n",
            "--------------------------------------------------\n",
            "Training fold 2 with 2177 features...\n",
            "Training until validation scores don't improve for 500 rounds.\n",
            "[500]\ttraining's binary_logloss: 0.209443\ttraining's amex_metric: 0.812314\tvalid_1's binary_logloss: 0.224672\tvalid_1's amex_metric: 0.786773\n",
            "[1000]\ttraining's binary_logloss: 0.188073\ttraining's amex_metric: 0.84348\tvalid_1's binary_logloss: 0.218369\tvalid_1's amex_metric: 0.792862\n",
            "[1500]\ttraining's binary_logloss: 0.172476\ttraining's amex_metric: 0.869247\tvalid_1's binary_logloss: 0.216858\tvalid_1's amex_metric: 0.794582\n",
            "[2000]\ttraining's binary_logloss: 0.159116\ttraining's amex_metric: 0.892405\tvalid_1's binary_logloss: 0.216231\tvalid_1's amex_metric: 0.794634\n",
            "Early stopping, best iteration is:\n",
            "[1567]\ttraining's binary_logloss: 0.170572\ttraining's amex_metric: 0.87289\tvalid_1's binary_logloss: 0.216738\tvalid_1's amex_metric: 0.795361\n",
            "Our fold 2 CV score is 0.7953610354781575\n",
            " \n",
            "--------------------------------------------------\n",
            "Training fold 3 with 2177 features...\n",
            "Training until validation scores don't improve for 500 rounds.\n",
            "[500]\ttraining's binary_logloss: 0.209258\ttraining's amex_metric: 0.812418\tvalid_1's binary_logloss: 0.225809\tvalid_1's amex_metric: 0.780491\n",
            "[1000]\ttraining's binary_logloss: 0.187812\ttraining's amex_metric: 0.844396\tvalid_1's binary_logloss: 0.219506\tvalid_1's amex_metric: 0.787997\n",
            "[1500]\ttraining's binary_logloss: 0.172219\ttraining's amex_metric: 0.870448\tvalid_1's binary_logloss: 0.217821\tvalid_1's amex_metric: 0.789225\n",
            "[2000]\ttraining's binary_logloss: 0.158867\ttraining's amex_metric: 0.893162\tvalid_1's binary_logloss: 0.217151\tvalid_1's amex_metric: 0.790603\n",
            "[2500]\ttraining's binary_logloss: 0.14697\ttraining's amex_metric: 0.913293\tvalid_1's binary_logloss: 0.216907\tvalid_1's amex_metric: 0.790515\n",
            "[3000]\ttraining's binary_logloss: 0.136227\ttraining's amex_metric: 0.930982\tvalid_1's binary_logloss: 0.216812\tvalid_1's amex_metric: 0.790825\n",
            "Early stopping, best iteration is:\n",
            "[2765]\ttraining's binary_logloss: 0.141125\ttraining's amex_metric: 0.92318\tvalid_1's binary_logloss: 0.216829\tvalid_1's amex_metric: 0.791419\n",
            "Our fold 3 CV score is 0.7914185963664591\n",
            " \n",
            "--------------------------------------------------\n",
            "Training fold 4 with 2177 features...\n",
            "Training until validation scores don't improve for 500 rounds.\n",
            "[500]\ttraining's binary_logloss: 0.209863\ttraining's amex_metric: 0.812148\tvalid_1's binary_logloss: 0.223109\tvalid_1's amex_metric: 0.7864\n",
            "[1000]\ttraining's binary_logloss: 0.188477\ttraining's amex_metric: 0.843159\tvalid_1's binary_logloss: 0.216722\tvalid_1's amex_metric: 0.792918\n",
            "[1500]\ttraining's binary_logloss: 0.172853\ttraining's amex_metric: 0.86871\tvalid_1's binary_logloss: 0.215102\tvalid_1's amex_metric: 0.794851\n",
            "[2000]\ttraining's binary_logloss: 0.159491\ttraining's amex_metric: 0.891907\tvalid_1's binary_logloss: 0.214434\tvalid_1's amex_metric: 0.79585\n",
            "[2500]\ttraining's binary_logloss: 0.147575\ttraining's amex_metric: 0.911953\tvalid_1's binary_logloss: 0.214108\tvalid_1's amex_metric: 0.797415\n",
            "[3000]\ttraining's binary_logloss: 0.13685\ttraining's amex_metric: 0.930337\tvalid_1's binary_logloss: 0.213928\tvalid_1's amex_metric: 0.79689\n",
            "[3500]\ttraining's binary_logloss: 0.127149\ttraining's amex_metric: 0.94568\tvalid_1's binary_logloss: 0.213877\tvalid_1's amex_metric: 0.797369\n",
            "Early stopping, best iteration is:\n",
            "[3176]\ttraining's binary_logloss: 0.133334\ttraining's amex_metric: 0.935938\tvalid_1's binary_logloss: 0.213828\tvalid_1's amex_metric: 0.797561\n",
            "Our fold 4 CV score is 0.7975607658813425\n",
            "Our out of folds CV score is 0.7957739558671801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CV: 0.79577"
      ],
      "metadata": {
        "id": "mbpn7U0YV8pp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LB: 0.796"
      ],
      "metadata": {
        "id": "fGkRQHQwbjUH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4_-5U7piblUG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}